# Frequently Asked Questions (FAQ)

Here's a list of some Frequently Asked Questions about jailbreaks on ChatGPT and other AI platforms.

<details>
<summary>What is a Jailbreak?</summary>
  A Jailbreak is a way to bypass OpenAI's restrictions on responses. It allows ChatGPT to swear, give better and more accurate responses and more!

  Here are some examples.

  ### Jailbroken Responses
  ![image](https://github.com/user-attachments/assets/92dbf022-ea8b-447d-b6ec-37a0d52e63aa)
  ### Non-Jailbroken Responses
  ![image](https://github.com/user-attachments/assets/1e61b0bd-263d-46bb-a133-f1144608a278)

</details>
<br>
<details>
  <summary>Does it get patched?</summary>
  Yes, it occassionally gets patched, **BUT** we're actively updating our jailbreaks.
</details>
<br>
<details>
  <summary>Is it against OpenAI ToS?</summary>
  Yes, Jailbreaking is against ToS.
</details>
<br>
<details>
  <summary>How do you use it?</summary>
  To use a prompt, simply copy the prompt, then paste it in.
</details>
